{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ee66c6",
   "metadata": {},
   "source": [
    "### Gensim là gì?\n",
    "Gensim là một thư viện xử lý ngôn ngữ tự nhiên mã nguồn mở phổ biến. Nó sử dụng các mô hình học thuật hàng đầu để thực hiện các nhiệm vụ phức tạp như xây dựng các véc-tơ từ và thực hiện xác định chủ đề và so sánh tài liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "584fa81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1320b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "                'I realzy liked the movie!',\n",
    "                'Awesome action scenes, but boring characters.',\n",
    "                'The movie was awful! I hate alien films.',\n",
    "                'Space is cool! I liked the movie.',\n",
    "                'More space films, please!',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc7196c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'movie', 'was', 'about', 'a', 'spaceship', 'and', 'aliens', '.'],\n",
       " ['i', 'realzy', 'liked', 'the', 'movie', '!'],\n",
       " ['awesome', 'action', 'scenes', ',', 'but', 'boring', 'characters', '.'],\n",
       " ['the', 'movie', 'was', 'awful', '!', 'i', 'hate', 'alien', 'films', '.'],\n",
       " ['space', 'is', 'cool', '!', 'i', 'liked', 'the', 'movie', '.'],\n",
       " ['more', 'space', 'films', ',', 'please', '!']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [word_tokenize((doc.lower())) for doc in my_documents]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9fbd714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91faafd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'about': 2,\n",
       " 'aliens': 3,\n",
       " 'and': 4,\n",
       " 'movie': 5,\n",
       " 'spaceship': 6,\n",
       " 'the': 7,\n",
       " 'was': 8,\n",
       " '!': 9,\n",
       " 'i': 10,\n",
       " 'liked': 11,\n",
       " 'realzy': 12,\n",
       " ',': 13,\n",
       " 'action': 14,\n",
       " 'awesome': 15,\n",
       " 'boring': 16,\n",
       " 'but': 17,\n",
       " 'characters': 18,\n",
       " 'scenes': 19,\n",
       " 'alien': 20,\n",
       " 'awful': 21,\n",
       " 'films': 22,\n",
       " 'hate': 23,\n",
       " 'cool': 24,\n",
       " 'is': 25,\n",
       " 'space': 26,\n",
       " 'more': 27,\n",
       " 'please': 28}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Danh sách tất cả các token và id tương ứng của chúng trong danh sách từ điển mới\n",
    "dictionary.token2id "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7182da",
   "metadata": {},
   "source": [
    "##### Tạo một corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e4ded75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
       " [(0, 1),\n",
       "  (5, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
       " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae874231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'movie', 'was', 'about', 'a', 'spaceship', 'and', 'aliens', '.']\n",
      "['i', 'realzy', 'liked', 'the', 'movie', '!']\n",
      "['awesome', 'action', 'scenes', ',', 'but', 'boring', 'characters', '.']\n",
      "['the', 'movie', 'was', 'awful', '!', 'i', 'hate', 'alien', 'films', '.']\n",
      "['space', 'is', 'cool', '!', 'i', 'liked', 'the', 'movie', '.']\n",
      "['more', 'space', 'films', ',', 'please', '!']\n"
     ]
    }
   ],
   "source": [
    "c = [print(doc) for doc in tokenized_docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199e6fcc",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cd6dd5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Men’s Football was introduced as a demonstration sport in the 1896 Olympic Games in Athens.'], ['It became an official Olympic sport in the 1908 Games in London.Nearly a century passed before women’s Football was included in the Olympic competition programme in the 1996 Games in Atlanta.Until the 1984 Olympic Games, participation was restricted to amateur players and Olympic Football was dominated by eastern European countries.'], ['After the inclusion of professional athletes at the Olympic Games, participation rules have been a subject of debate between the International Olympic Committee and FIFA. As a result, new regulations were established specifying the age of participant athletes. The new regulations have enabled many African countries to display a rich pool of Football talents.'])\n"
     ]
    }
   ],
   "source": [
    "text = ['Men’s Football was introduced as a demonstration sport in the 1896 Olympic Games in Athens.'], ['It became an official Olympic sport in the 1908 Games in London.Nearly a century passed before women’s Football was included in the Olympic competition programme in the 1996 Games in Atlanta.Until the 1984 Olympic Games, participation was restricted to amateur players and Olympic Football was dominated by eastern European countries.'],['After the inclusion of professional athletes at the Olympic Games, participation rules have been a subject of debate between the International Olympic Committee and FIFA. As a result, new regulations were established specifying the age of participant athletes. The new regulations have enabled many African countries to display a rich pool of Football talents.']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9cccbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text = [sent for sent in sent_tokenize(text)]re.sub(r'[^\\w]', ' ', text)\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "75f2379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6e6c2db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['men', 'football', 'introduced', 'demonstration', 'sport', '1896', 'olympic', 'games', 'athens', 'became', 'official', 'olympic', 'sport', '1908', 'games', 'london', 'nearly', 'century', 'passed', 'women', 'football', 'included', 'olympic', 'competition', 'programme', '1996', 'games', 'atlanta', '1984', 'olympic', 'games', 'participation', 'restricted', 'amateur', 'players', 'olympic', 'football', 'dominated', 'eastern', 'european', 'countries', 'inclusion', 'professional', 'athletes', 'olympic', 'games', 'participation', 'rules', 'subject', 'debate', 'international', 'olympic', 'committee', 'fifa', 'result', 'new', 'regulations', 'established', 'specifying', 'age', 'participant', 'athletes', 'new', 'regulations', 'enabled', 'many', 'african', 'countries', 'display', 'rich', 'pool', 'football', 'talents']\n"
     ]
    }
   ],
   "source": [
    "# token = [word for word in word_tokenize(text.lower())]\n",
    "no_stops = [word for word in token\n",
    "          if word not in stopwords.words('english')]\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# articles = [wordnet_lemmatizer.lemmatize(word) for word in no_stops]\n",
    "print(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9806f36c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [126]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m \u001b[43mDictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticles\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\gensim\\corpora\\dictionary.py:78\u001b[0m, in \u001b[0;36mDictionary.__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nnz \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprune_at\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprune_at\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m         msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     82\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\gensim\\corpora\\dictionary.py:204\u001b[0m, in \u001b[0;36mDictionary.add_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    201\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madding document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, docno, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# update Dictionary with the document\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc2bow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[0;32m    206\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\gensim\\corpora\\dictionary.py:241\u001b[0m, in \u001b[0;36mDictionary.doc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m\"\"\"Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(document, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc2bow expects an array of unicode tokens on input, not a single string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Construct (word, frequency) mapping.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m counter \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70954192",
   "metadata": {},
   "outputs": [],
   "source": [
    "computer = dictionary.token2id.get('computer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
